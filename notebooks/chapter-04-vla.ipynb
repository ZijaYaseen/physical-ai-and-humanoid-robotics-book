{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Vision-Language-Action (VLA) Models\n",
    "\n",
    "This notebook explores Vision-Language-Action (VLA) models for robotics. VLA models connect visual perception, natural language understanding, and robotic action to enable robots to follow natural language commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration cell - select execution mode\n",
    "EXECUTION_MODE = \"simulation\"  # Options: \"hardware\", \"simulation\"\n",
    "\n",
    "print(f\"VLA Chapter 4: Running in {EXECUTION_MODE} mode\")\n",
    "print(\"Initializing VLA environment...\")\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"VLA environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Vision-Language-Action (VLA) Models\n",
    "\n",
    "VLA models represent a significant advancement in robotics, enabling robots to understand natural language instructions and execute complex tasks by connecting visual perception with action. In this notebook, we'll explore the architecture and applications of VLA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock VLA model for simulation mode\n",
    "if EXECUTION_MODE == \"simulation\":\n",
    "    print(\"Using VLA simulation mode - no physical hardware required\")\n",
    "    \n",
    "    # Mock VLA model class\n",
    "    class MockVLAModel:\n",
    "        def __init__(self, model_name=\"mock-vla\"):\n",
    "            self.model_name = model_name\n",
    "            self.is_loaded = False\n",
    "            print(f\"Mock VLA Model '{model_name}' initialized\")\n",
    "        \n",
    "        def load_model(self):\n",
    "            print(f\"Loading mock VLA model: {self.model_name}\")\n",
    "            # Simulate model loading time\n",
    "            time.sleep(0.5)\n",
    "            self.is_loaded = True\n",
    "            print(\"Mock VLA model loaded successfully\")\n",
    "        \n",
    "        def process_instruction(self, image, instruction):\n",
    "            if not self.is_loaded:\n",
    "                self.load_model()\n",
    "            \n",
    "            print(f\"Processing instruction: '{instruction}'\")\n",
    "            print(f\"With image of shape: {getattr(image, 'shape', 'unknown')}\")\n",
    "            \n",
    "            # Simulate VLA processing\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "            # Mock action generation\n",
    "            actions = self._generate_mock_actions(instruction)\n",
    "            \n",
    "            return actions\n",
    "        \n",
    "        def _generate_mock_actions(self, instruction):\n",
    "            # Generate mock actions based on the instruction\n",
    "            instruction_lower = instruction.lower()\n",
    "            \n",
    "            if \"pick\" in instruction_lower or \"grasp\" in instruction_lower:\n",
    "                return [{\n",
    "                    'action_type': 'grasp',\n",
    "                    'object': 'object',\n",
    "                    'position': [0.5, 0.3, 0.1],\n",
    "                    'confidence': 0.9\n",
    "                }]\n",
    "            elif \"move\" in instruction_lower or \"go\" in instruction_lower:\n",
    "                return [{\n",
    "                    'action_type': 'navigate',\n",
    "                    'target': 'location',\n",
    "                    'position': [1.0, 0.0, 0.0],\n",
    "                    'confidence': 0.85\n",
    "                }]\n",
    "            elif \"push\" in instruction_lower or \"press\" in instruction_lower:\n",
    "                return [{\n",
    "                    'action_type': 'push',\n",
    "                    'object': 'button',\n",
    "                    'position': [0.2, 0.8, 0.05],\n",
    "                    'confidence': 0.88\n",
    "                }]\n",
    "            else:\n",
    "                return [{\n",
    "                    'action_type': 'unknown',\n",
    "                    'description': instruction,\n",
    "                    'confidence': 0.6\n",
    "                }]\n",
    "    \n",
    "    # Create mock VLA components\n",
    "    vla_model = MockVLAModel()\n",
    "    print(\"Mock VLA components initialized\")\n",
    "else:\n",
    "    print(\"Using VLA hardware mode - connecting to VLA platform\")\n",
    "    # In a real environment, we would import actual VLA packages\n",
    "    # import vla_models\n",
    "    # from vla_models import VLAModel\n",
    "    \n",
    "    # For this simulation, we'll use the mock components\n",
    "    vla_model = MockVLAModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Simulated Environment Data\n",
    "\n",
    "Let's create simulated environment data that our VLA model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simulated environment image\n",
    "def create_simulated_environment_image(width=640, height=480, channels=3):\n",
    "    \"\"\"Create a simulated environment image with objects for VLA processing\"\"\"\n",
    "    image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "    \n",
    "    # Create a simple scene with objects\n",
    "    # Table\n",
    "    image[300:480, :, :] = [101, 67, 33]  # Brown table\n",
    "    \n",
    "    # Red cup\n",
    "    center_x, center_y = 150, 250\n",
    "    radius = 30\n",
    "    y, x = np.ogrid[:height, :width]\n",
    "    mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "    image[mask] = [255, 0, 0]  # Red cup\n",
    "    \n",
    "    # Blue box\n",
    "    image[200:280, 400:480, :] = [0, 0, 255]  # Blue box\n",
    "    \n",
    "    # Green bottle\n",
    "    image[150:250, 300:320, :] = [0, 255, 0]  # Green bottle\n",
    "    \n",
    "    # Add some texture to the table\n",
    "    for i in range(300, 480, 20):\n",
    "        image[i:i+2, :, :] = [85, 55, 28]  # Darker wood lines\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Create simulated environment data\n",
    "environment_image = create_simulated_environment_image()\n",
    "print(f\"Simulated environment image created with shape: {environment_image.shape}\")\n",
    "\n",
    "# Visualize the simulated environment\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(environment_image)\n",
    "plt.title('Simulated Environment for VLA Processing')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing Natural Language Instructions\n",
    "\n",
    "Now let's test our VLA model with various natural language instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define various natural language instructions to test\n",
    "instructions = [\n",
    "    \"Pick up the red cup\",\n",
    "    \"Move to the blue box\",\n",
    "    \"Push the green button\",\n",
    "    \"Go to the table\",\n",
    "    \"Grasp the object on the left\"\n",
    "]\n",
    "\n",
    "# Process each instruction with the VLA model\n",
    "print(\"Processing natural language instructions with VLA model...\")\n",
    "\n",
    "all_results = []\n",
    "for i, instruction in enumerate(instructions):\n",
    "    print(f\"\\nInstruction {i+1}: '{instruction}'\")\n",
    "    \n",
    "    # Process the instruction with the VLA model\n",
    "    actions = vla_model.process_instruction(environment_image, instruction)\n",
    "    \n",
    "    print(f\"Generated actions: {actions}\")\n",
    "    all_results.append({\n",
    "        'instruction': instruction,\n",
    "        'actions': actions\n",
    "    })\n",
    "\n",
    "# Display summary of all results\n",
    "print(f\"\\nProcessed {len(instructions)} instructions with VLA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VLA Model Architecture Visualization\n",
    "\n",
    "Let's visualize how the VLA model connects vision, language, and action components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of the VLA architecture\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Define positions for the VLA components\n",
    "positions = {\n",
    "    'vision': (0, 0.5),\n",
    "    'language': (0, 0.8),\n",
    "    'fusion': (0.5, 0.65),\n",
    "    'action': (1, 0.5),\n",
    "    'robot': (1, 0.2)\n",
    "}\n",
    "\n",
    "# Draw components\n",
    "for component, pos in positions.items():\n",
    "    x, y = pos\n",
    "    \n",
    "    if component == 'vision':\n",
    "        # Draw camera icon\n",
    "        circle = plt.Circle((x, y), 0.05, color='blue', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y-0.08, 'Vision', ha='center', va='center', fontweight='bold')\n",
    "        ax.text(x, y-0.12, '(Image Input)', ha='center', va='center', fontsize=8)\n",
    "    elif component == 'language':\n",
    "        # Draw speech bubble\n",
    "        circle = plt.Circle((x, y), 0.05, color='green', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y-0.08, 'Language', ha='center', va='center', fontweight='bold')\n",
    "        ax.text(x, y-0.12, '(Text Input)', ha='center', va='center', fontsize=8)\n",
    "    elif component == 'fusion':\n",
    "        # Draw fusion center\n",
    "        circle = plt.Circle((x, y), 0.07, color='purple', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y-0.1, 'Fusion', ha='center', va='center', fontweight='bold')\n",
    "        ax.text(x, y-0.15, '(VLA Model)', ha='center', va='center', fontsize=8)\n",
    "    elif component == 'action':\n",
    "        # Draw action output\n",
    "        circle = plt.Circle((x, y), 0.05, color='orange', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y-0.08, 'Action', ha='center', va='center', fontweight='bold')\n",
    "        ax.text(x, y-0.12, '(Motor Commands)', ha='center', va='center', fontsize=8)\n",
    "    elif component == 'robot':\n",
    "        # Draw robot\n",
    "        circle = plt.Circle((x, y), 0.05, color='red', alpha=0.7)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x, y-0.08, 'Robot', ha='center', va='center', fontweight='bold')\n",
    "        ax.text(x, y-0.12, '(Execution)', ha='center', va='center', fontsize=8)\n",
    "\n",
    "# Draw connections\n",
    "connections = [\n",
    "    (positions['vision'], positions['fusion']),\n",
    "    (positions['language'], positions['fusion']),\n",
    "    (positions['fusion'], positions['action']),\n",
    "    (positions['action'], positions['robot'])\n",
    "]\n",
    "\n",
    "for start, end in connections:\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='gray', alpha=0.7))\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_xlim(-0.2, 1.2)\n",
    "ax.set_ylim(-0.2, 1.2)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Vision-Language-Action (VLA) Model Architecture', fontsize=16, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"VLA architecture visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Modal Integration\n",
    "\n",
    "Let's explore how VLA models integrate visual and language information to generate appropriate actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to simulate multi-modal integration\n",
    "class VLAIntegrationSimulator:\n",
    "    def __init__(self):\n",
    "        self.name = \"VLA Multi-Modal Integration Simulator\"\n",
    "        print(f\"{self.name} initialized\")\n",
    "    \n",
    "    def integrate_modalities(self, visual_data, language_instruction):\n",
    "        print(f\"Integrating visual data and language instruction:\")\n",
    "        print(f\"  Visual: Scene with objects at various positions\")\n",
    "        print(f\"  Language: '{language_instruction}'\")\n",
    "        \n",
    "        # Simulate attention mechanism\n",
    "        attention_map = self._create_attention_map(visual_data, language_instruction)\n",
    "        \n",
    "        # Identify relevant objects based on instruction\n",
    "        relevant_objects = self._identify_objects(visual_data, language_instruction)\n",
    "        \n",
    "        # Generate action plan\n",
    "        action_plan = self._generate_action_plan(relevant_objects, language_instruction)\n",
    "        \n",
    "        return {\n",
    "            'attention_map': attention_map,\n",
    "            'relevant_objects': relevant_objects,\n",
    "            'action_plan': action_plan\n",
    "        }\n",
    "    \n",
    "    def _create_attention_map(self, visual_data, instruction):\n",
    "        # Simulate attention map generation\n",
    "        height, width = visual_data.shape[:2]\n",
    "        attention_map = np.zeros((height, width))\n",
    "        \n",
    "        # Create attention based on instruction keywords\n",
    "        if \"red\" in instruction.lower():\n",
    "            # Focus on red areas\n",
    "            red_mask = np.all(visual_data == [255, 0, 0], axis=-1)\n",
    "            attention_map[red_mask] = 1.0\n",
    "        elif \"blue\" in instruction.lower():\n",
    "            # Focus on blue areas\n",
    "            blue_mask = np.all(visual_data == [0, 0, 255], axis=-1)\n",
    "            attention_map[blue_mask] = 1.0\n",
    "        elif \"green\" in instruction.lower():\n",
    "            # Focus on green areas\n",
    "            green_mask = np.all(visual_data == [0, 255, 0], axis=-1)\n",
    "            attention_map[green_mask] = 1.0\n",
    "        else:\n",
    "            # General attention\n",
    "            attention_map[200:400, 200:400] = 0.5  # Center region\n",
    "        \n",
    "        return attention_map\n",
    "    \n",
    "    def _identify_objects(self, visual_data, instruction):\n",
    "        # Simulate object identification\n",
    "        objects = []\n",
    "        \n",
    "        # Check for red cup\n",
    "        red_mask = np.all(visual_data == [255, 0, 0], axis=-1)\n",
    "        if np.any(red_mask):\n",
    "            y_coords, x_coords = np.where(red_mask)\n",
    "            center_x = int(np.mean(x_coords))\n",
    "            center_y = int(np.mean(y_coords))\n",
    "            objects.append({\n",
    "                'name': 'red_cup',\n",
    "                'color': 'red',\n",
    "                'position': (center_x, center_y),\n",
    "                'bbox': [center_x-30, center_y-30, center_x+30, center_y+30]\n",
    "            })\n",
    "        \n",
    "        # Check for blue box\n",
    "        blue_mask = np.all(visual_data == [0, 0, 255], axis=-1)\n",
    "        if np.any(blue_mask):\n",
    "            y_coords, x_coords = np.where(blue_mask)\n",
    "            center_x = int(np.mean(x_coords))\n",
    "            center_y = int(np.mean(y_coords))\n",
    "            objects.append({\n",
    "                'name': 'blue_box',\n",
    "                'color': 'blue',\n",
    "                'position': (center_x, center_y),\n",
    "                'bbox': [center_x-40, center_y-40, center_x+40, center_y+40]\n",
    "            })\n",
    "        \n",
    "        # Check for green bottle\n",
    "        green_mask = np.all(visual_data == [0, 255, 0], axis=-1)\n",
    "        if np.any(green_mask):\n",
    "            y_coords, x_coords = np.where(green_mask)\n",
    "            center_x = int(np.mean(x_coords))\n",
    "            center_y = int(np.mean(y_coords))\n",
    "            objects.append({\n",
    "                'name': 'green_bottle',\n",
    "                'color': 'green',\n",
    "                'position': (center_x, center_y),\n",
    "                'bbox': [center_x-10, center_y-50, center_x+10, center_y+50]\n",
    "            })\n",
    "        \n",
    "        return objects\n",
    "    \n",
    "    def _generate_action_plan(self, objects, instruction):\n",
    "        # Generate action plan based on objects and instruction\n",
    "        instruction_lower = instruction.lower()\n",
    "        \n",
    "        if \"pick\" in instruction_lower or \"grasp\" in instruction_lower:\n",
    "            # Find object to pick based on color mentioned in instruction\n",
    "            for obj in objects:\n",
    "                if obj['color'] in instruction_lower or obj['name'] in instruction_lower:\n",
    "                    return {\n",
    "                        'action': 'grasp',\n",
    "                        'target_object': obj['name'],\n",
    "                        'position': obj['position'],\n",
    "                        'sequence': ['approach', 'grasp', 'lift']\n",
    "                    }\n",
    "        elif \"move\" in instruction_lower or \"go\" in instruction_lower:\n",
    "            # Move to the first object\n",
    "            if objects:\n",
    "                return {\n",
    "                    'action': 'navigate',\n",
    "                    'target_object': objects[0]['name'],\n",
    "                    'position': objects[0]['position'],\n",
    "                    'sequence': ['plan_path', 'move_to_location']\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'action': 'unknown',\n",
    "            'target_object': 'none',\n",
    "            'position': (0, 0),\n",
    "            'sequence': ['wait_for_clarification']\n",
    "        }\n",
    "\n",
    "# Create VLA integration simulator\n",
    "vla_simulator = VLAIntegrationSimulator()\n",
    "print(\"VLA Integration Simulator created\")\n",
    "\n",
    "# Test integration with different instructions\n",
    "test_instructions = [\n",
    "    \"Pick up the red cup\",\n",
    "    \"Go to the blue box\",\n",
    "    \"Move near the green bottle\"\n",
    "]\n",
    "\n",
    "for instruction in test_instructions:\n",
    "    print(f\"\\n--- Processing: '{instruction}' ---\")\n",
    "    result = vla_simulator.integrate_modalities(environment_image, instruction)\n",
    "    \n",
    "    print(f\"Relevant objects: {[obj['name'] for obj in result['relevant_objects']]}\")\n",
    "    print(f\"Action plan: {result['action_plan']['action']} -> {result['action_plan']['target_object']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. VLA Action Execution Simulation\n",
    "\n",
    "Let's simulate how the VLA model's generated actions would be executed by a robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a robot simulator to execute VLA actions\n",
    "class VLARobotSimulator:\n",
    "    def __init__(self):\n",
    "        self.name = \"VLA Robot Simulator\"\n",
    "        self.position = (0.5, 0.5)  # Starting position\n",
    "        self.holding_object = None\n",
    "        print(f\"{self.name} initialized at position {self.position}\")\n",
    "    \n",
    "    def execute_action(self, action_plan):\n",
    "        print(f\"\\nExecuting action: {action_plan['action']} -> {action_plan['target_object']}\")\n",
    "        \n",
    "        if action_plan['action'] == 'grasp':\n",
    "            return self._execute_grasp(action_plan)\n",
    "        elif action_plan['action'] == 'navigate':\n",
    "            return self._execute_navigation(action_plan)\n",
    "        else:\n",
    "            print(f\"Unknown action: {action_plan['action']}\")\n",
    "            return False\n",
    "    \n",
    "    def _execute_grasp(self, action_plan):\n",
    "        target_pos = action_plan['position']\n",
    "        print(f\"Approaching target at {target_pos}\")\n",
    "        \n",
    "        # Simulate approach\n",
    "        time.sleep(0.3)\n",
    "        print(\"Aligned with target object\")\n",
    "        \n",
    "        # Simulate grasp\n",
    "        print(\"Grasping object\")\n",
    "        self.holding_object = action_plan['target_object']\n",
    "        print(f\"Successfully grasped {self.holding_object}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _execute_navigation(self, action_plan):\n",
    "        target_pos = action_plan['position']\n",
    "        print(f\"Planning path to {target_pos}\")\n",
    "        \n",
    "        # Simulate path planning\n",
    "        time.sleep(0.2)\n",
    "        print(\"Path planned and executing\")\n",
    "        \n",
    "        # Update position\n",
    "        self.position = target_pos\n",
    "        print(f\"Reached position {self.position}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_state(self):\n",
    "        return {\n",
    "            'position': self.position,\n",
    "            'holding': self.holding_object\n",
    "        }\n",
    "\n",
    "# Create robot simulator\n",
    "robot_sim = VLARobotSimulator()\n",
    "print(\"VLA Robot Simulator created\")\n",
    "\n",
    "# Execute the action plans generated by the VLA integration\n",
    "for i, result in enumerate(all_results):\n",
    "    print(f\"\\n=== Executing Plan {i+1}: {result['instruction']} ===\")\n",
    "    \n",
    "    # Get the action plan from the VLA integration\n",
    "    action_plan = result['actions'][0] if result['actions'] else {'action': 'unknown', 'target_object': 'none', 'position': (0, 0)}\n",
    "    \n",
    "    # Execute the action\n",
    "    success = robot_sim.execute_action(action_plan)\n",
    "    \n",
    "    # Print robot state\n",
    "    state = robot_sim.get_state()\n",
    "    print(f\"Robot state: Position={state['position']}, Holding={state['holding']}\")\n",
    "\n",
    "print(f\"\\nAll VLA action plans executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. VLA Model Training Concepts\n",
    "\n",
    "Let's explore the concepts behind training VLA models with multimodal datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the VLA training process\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Training data distribution\n",
    "num_samples = 1000\n",
    "vision_features = np.random.randn(num_samples, 512)  # Simulated vision features\n",
    "language_features = np.random.randn(num_samples, 512)  # Simulated language features\n",
    "action_features = np.random.randn(num_samples, 64)  # Simulated action features\n",
    "\n",
    "# Simulate correlation between modalities\n",
    "correlated_data = vision_features[:, :100] + language_features[:, :100] + np.random.randn(num_samples, 100) * 0.1\n",
    "\n",
    "ax1.scatter(correlated_data[:, 0], correlated_data[:, 1], alpha=0.6)\n",
    "ax1.set_title('Correlated Features in VLA Training Data', fontweight='bold')\n",
    "ax1.set_xlabel('Feature Dimension 1')\n",
    "ax1.set_ylabel('Feature Dimension 2')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training loss over epochs\n",
    "epochs = 50\n",
    "train_loss = 2.0 * np.exp(-np.arange(epochs) * 0.1) + 0.1 + 0.1 * np.random.randn(epochs)\n",
    "val_loss = 2.2 * np.exp(-np.arange(epochs) * 0.08) + 0.15 + 0.12 * np.random.randn(epochs)\n",
    "\n",
    "ax2.plot(train_loss, label='Training Loss', linewidth=2)\n",
    "ax2.plot(val_loss, label='Validation Loss', linewidth=2)\n",
    "ax2.set_title('VLA Model Training: Loss Over Epochs', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"VLA training visualization complete\")\n",
    "print(\"Note: In real VLA training, models learn to connect vision, language, and action modalities\")\n",
    "print(\"through large datasets of human demonstrations and multimodal supervision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "1. Vision-Language-Action (VLA) model architecture\n",
    "2. Processing natural language instructions with visual input\n",
    "3. Multi-modal integration of vision and language\n",
    "4. Action generation and robot execution simulation\n",
    "5. VLA model training concepts\n",
    "6. Visualization of VLA architecture and processes\n",
    "\n",
    "VLA models represent a significant advancement in robotics, enabling robots to understand natural language commands and execute complex tasks by connecting visual perception with action. These models are crucial for developing more intuitive and flexible robotic systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "print(\"\\nChapter 4 complete! You've learned about Vision-Language-Action models.\")\n",
    "print(\"VLA concepts covered:\")\n",
    "print(\"- VLA model architecture\")\n",
    "print(\"- Natural language instruction processing\")\n",
    "print(\"- Multi-modal integration\")\n",
    "print(\"- Action generation and execution\")\n",
    "print(\"- Training concepts\")\n",
    "\n",
    "print(f\"\\nVLA model simulation completed successfully!\")\n",
    "print(f\"Processed {len(instructions)} natural language instructions\")\n",
    "print(f\"Executed {len(all_results)} action plans\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}